{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbacacab-902e-4219-ad7d-849aca55bd0d",
   "metadata": {},
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b53f997-8e7a-4e08-b75e-c91fec6131b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "query = []\n",
    "file_path = ['datasets/Boolean_문장.csv']\n",
    "\n",
    "for i in range(len(file_path)):\n",
    "    with open(file_path[i], mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for article in query:\n",
    "            writer.writerow([article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d28b89-6b31-4d2d-8a58-4d2148554ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def read_csv_to_dict(file_path, key_names):\n",
    "    result_dict = {}\n",
    "    for i, path in enumerate(file_path):\n",
    "        file_name = key_names[i]  \n",
    "        if os.path.isfile(path):\n",
    "            with open(path, mode='r', newline='', encoding='utf-8') as file:\n",
    "                reader = csv.reader(file)\n",
    "                titles = set()\n",
    "                for row in reader:\n",
    "                    titles.add(row[0])  # 첫 번째 col은 title\n",
    "                result_dict[file_name] = titles\n",
    "        else:\n",
    "            print(f\"cannot find the file: {path}\")\n",
    "    return result_dict\n",
    "\n",
    "label_data = ['datasets/총선.csv', 'datasets/선거.csv', 'datasets/제22대_국회의원선거.csv', 'datasets/4월_10일_투표.csv', 'datasets/문장.csv']\n",
    "boolean_data = ['datasets/Boolean_총선.csv', 'datasets/Boolean_선거.csv', 'datasets/Boolean_제22대.csv', 'datasets/Boolean_410투표.csv', 'datasets/Boolean_문장.csv']\n",
    "vector_data = ['datasets/Vector_총선.csv', 'datasets/Vector_선거.csv', 'datasets/Vector_제22대.csv', 'datasets/Vector_410투표.csv', 'datasets/Vector_문장.csv']\n",
    "\n",
    "keys = ['총선', '선거', '제22대_국회의원선거', '4월_10일_투표', '문장']\n",
    "\n",
    "ground_truths = read_csv_to_dict(label_data, keys)\n",
    "boolean_results = read_csv_to_dict(boolean_data, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b0f56-8437-41fa-b1c0-9450886168b1",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de42008c-b623-46bf-82e3-ab5e92f49a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. precision\n",
    "# 2. recall\n",
    "# 3. f1 score\n",
    "# 4. average\n",
    "def calculate_precision_recall(ground_truth, model_results):\n",
    "    true_positives = ground_truth.intersection(model_results)\n",
    "    precision = len(true_positives) / len(model_results) if model_results else 0\n",
    "    recall = len(true_positives) / len(ground_truth) if ground_truth else 0\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654b0816-3015-42d8-a312-76eb08e87057",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m gt \u001b[38;5;241m=\u001b[39m ground_truths[key]\n\u001b[0;32m      5\u001b[0m boolean \u001b[38;5;241m=\u001b[39m boolean_results[key]\n\u001b[1;32m----> 6\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43mvector_results\u001b[49m[key]  \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# boolean model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m precision_b, recall_b \u001b[38;5;241m=\u001b[39m calculate_precision_recall(gt, boolean)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector_results' is not defined"
     ]
    }
   ],
   "source": [
    "results_b, results_v = [], []\n",
    "\n",
    "for key in ground_truths.keys():\n",
    "    gt = ground_truths[key]\n",
    "    boolean = boolean_results[key]\n",
    "    vector = vector_results[key]  \n",
    "\n",
    "    # boolean model\n",
    "    precision_b, recall_b = calculate_precision_recall(gt, boolean)\n",
    "    f1_score_b = calculate_f1_score(precision_b, recall_b)\n",
    "    print(f\"==================== result of boolean model [ '{key}' ] ====================\")\n",
    "    print(f\"Precision: {precision_b:.2f}, Recall: {recall_b:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score_b:.2f}\\n\")\n",
    "    results_b.append((precision_b, recall_b, f1_score_b))\n",
    "\n",
    "    # vector model\n",
    "    precision_v, recall_v = calculate_precision_recall(gt, vector)\n",
    "    f1_score_v = calculate_f1_score(precision_v, recall_v)\n",
    "    print(f\"==================== result of vector model [ '{key}' ] ====================\")\n",
    "    print(f\"Precision: {precision_v:.2f}, Recall: {recall_v:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score_v:.2f}\\n\")\n",
    "    results_v.append((precision_v, recall_v, f1_score_v))\n",
    "\n",
    "# Average for boolean model\n",
    "average_precision_b = sum([result[0] for result in results_b]) / len(results_b)\n",
    "average_recall_b = sum([result[1] for result in results_b]) / len(results_b)\n",
    "average_f1_score_b = sum([result[2] for result in results_b]) / len(results_b)\n",
    "\n",
    "print(f\"Average Precision of boolean model: {average_precision_b:.2f}\")\n",
    "print(f\"Average Recall of boolean model: {average_recall_b:.2f}\")\n",
    "print(f\"Average F1 Score of boolean model: {average_f1_score_b:.2f}\")\n",
    "\n",
    "average_results_b = [average_precision_b, average_recall_b, average_f1_score_b]\n",
    "\n",
    "# Average for vector model\n",
    "average_precision_v = sum([result[0] for result in results_v]) / len(results_v)\n",
    "average_recall_v = sum([result[1] for result in results_v]) / len(results_v)\n",
    "average_f1_score_v = sum([result[2] for result in results_v]) / len(results_v)\n",
    "\n",
    "print(f\"Average Precision of vector model: {average_precision_v:.2f}\")\n",
    "print(f\"Average Recall of vector model: {average_recall_v:.2f}\")\n",
    "print(f\"Average F1 Score of vector model: {average_f1_score_v:.2f}\")\n",
    "\n",
    "average_results_v = [average_precision_v, average_recall_v, average_f1_score_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844aa52e-b99d-46d8-a9bc-f2057ad59eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. temporary relevance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
