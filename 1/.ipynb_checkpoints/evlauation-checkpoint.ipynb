{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de490763-3230-476f-aa59-0348d94b090a",
   "metadata": {},
   "source": [
    "# svd 50 + preprocess(쿼리도) + 유사도 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e9bd046-c010-4967-afec-2963190ceda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "########################################################### 파일 읽기\n",
    "def read_csv_to_dict(file_paths, keys):\n",
    "    data_dict = {}\n",
    "    \n",
    "    for file_path, key in zip(file_paths, keys):\n",
    "        articles = []\n",
    "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                titles = row['Title']\n",
    "                if len(row) > 1:                \n",
    "                    date_with_time = row['Date']\n",
    "                    date_obj = datetime.strptime(date_with_time, \"%Y/%m/%d %H:%M\")                \n",
    "                    # Extract only the date part (stripping out the time)\n",
    "                    date_only = date_obj.strftime(\"%Y/%m/%d\")\n",
    "                else: \n",
    "                    date_only = None\n",
    "                article = {\n",
    "                    'title': titles,\n",
    "                    'date': date_only\n",
    "                }\n",
    "                articles.append(article)\n",
    "        \n",
    "        data_dict[key] = articles\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "########################################################### precision/recall/f1\n",
    "def calculate_precision_recall(ground_truth, model_results):\n",
    "    true_positives = set([article['title'] for article in ground_truth]).intersection(set([article['title'] for article in model_results]))\n",
    "    precision = len(true_positives) / len(model_results) if model_results else 0\n",
    "    recall = len(true_positives) / len(ground_truth) if ground_truth else 0\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "def evaluate_1(ground_truths, boolean_results, vector_results):\n",
    "    results_b = []\n",
    "    results_v = []\n",
    "\n",
    "    for key in ground_truths.keys():\n",
    "        ground_truth = ground_truths[key]\n",
    "        boolean = boolean_results[key]\n",
    "        vector = vector_results[key]\n",
    "        \n",
    "        # Boolean model evaluation\n",
    "        precision_b, recall_b = calculate_precision_recall(ground_truth, boolean)\n",
    "        f1_score_b = calculate_f1_score(precision_b, recall_b)\n",
    "        print(f\"==================== result of boolean model [ '{key}' ] ====================\")\n",
    "        print(f\"Precision: {precision_b:.2f}, Recall: {recall_b:.2f}\")\n",
    "        print(f\"F1 Score: {f1_score_b:.2f}\\n\")\n",
    "        results_b.append((precision_b, recall_b, f1_score_b))\n",
    "\n",
    "        # Vector model evaluation\n",
    "        precision_v, recall_v = calculate_precision_recall(ground_truth, vector)\n",
    "        f1_score_v = calculate_f1_score(precision_v, recall_v)\n",
    "        print(f\"==================== result of vector model [ '{key}' ] ====================\")\n",
    "        print(f\"Precision: {precision_v:.2f}, Recall: {recall_v:.2f}\")\n",
    "        print(f\"F1 Score: {f1_score_v:.2f}\\n\")\n",
    "        results_v.append((precision_v, recall_v, f1_score_v))\n",
    "\n",
    "    # Average for boolean model\n",
    "    average_precision_b = sum([result[0] for result in results_b]) / len(results_b)\n",
    "    average_recall_b = sum([result[1] for result in results_b]) / len(results_b)\n",
    "    average_f1_score_b = sum([result[2] for result in results_b]) / len(results_b)\n",
    "\n",
    "    print(f\"Average Precision of boolean model: {average_precision_b:.2f}\")\n",
    "    print(f\"Average Recall of boolean model: {average_recall_b:.2f}\")\n",
    "    print(f\"Average F1 Score of boolean model: {average_f1_score_b:.2f}\\n\")\n",
    "\n",
    "    # Average for vector model\n",
    "    average_precision_v = sum([result[0] for result in results_v]) / len(results_v)\n",
    "    average_recall_v = sum([result[1] for result in results_v]) / len(results_v)\n",
    "    average_f1_score_v = sum([result[2] for result in results_v]) / len(results_v)\n",
    "\n",
    "    print(f\"Average Precision of vector model: {average_precision_v:.2f}\")\n",
    "    print(f\"Average Recall of vector model: {average_recall_v:.2f}\")\n",
    "    print(f\"Average F1 Score of vector model: {average_f1_score_v:.2f}\")\n",
    "\n",
    "    return (average_precision_b, average_recall_b, average_f1_score_b), (average_precision_v, average_recall_v, average_f1_score_v), (results_b, results_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30fe0fda-3a67-4fda-87fc-4104945e1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = ['datasets/총선.csv', 'datasets/선거.csv', 'datasets/제22대_국회의원선거.csv', 'datasets/4월_10일_투표.csv']\n",
    "boolean_data = ['datasets/Boolean_총선.csv', 'datasets/Boolean_선거.csv', 'datasets/Boolean_제22대.csv', 'datasets/Boolean_410투표.csv']\n",
    "vector_data = ['datasets/Vector_총선.csv', 'datasets/Vector_선거.csv', 'datasets/Vector_제22대.csv', 'datasets/Vector_410투표.csv']\n",
    "\n",
    "keys = ['총선', '선거', '제22대_국회의원선거', '4월_10일_투표']\n",
    "\n",
    "boolean_results = read_csv_to_dict(boolean_data, keys)\n",
    "vector_results = read_csv_to_dict(vector_data, keys)\n",
    "ground_truths = read_csv_to_dict(label_data, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2c8515cb-9a6a-4dbe-ad2e-9351ed14b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== result of boolean model [ '총선' ] ====================\n",
      "Precision: 0.96, Recall: 0.62\n",
      "F1 Score: 0.75\n",
      "\n",
      "==================== result of vector model [ '총선' ] ====================\n",
      "Precision: 0.72, Recall: 0.76\n",
      "F1 Score: 0.74\n",
      "\n",
      "==================== result of boolean model [ '선거' ] ====================\n",
      "Precision: 0.93, Recall: 0.33\n",
      "F1 Score: 0.49\n",
      "\n",
      "==================== result of vector model [ '선거' ] ====================\n",
      "Precision: 0.82, Recall: 0.57\n",
      "F1 Score: 0.67\n",
      "\n",
      "==================== result of boolean model [ '제22대_국회의원선거' ] ====================\n",
      "Precision: 0.47, Recall: 0.22\n",
      "F1 Score: 0.30\n",
      "\n",
      "==================== result of vector model [ '제22대_국회의원선거' ] ====================\n",
      "Precision: 0.09, Recall: 0.91\n",
      "F1 Score: 0.16\n",
      "\n",
      "==================== result of boolean model [ '4월_10일_투표' ] ====================\n",
      "Precision: 1.00, Recall: 0.05\n",
      "F1 Score: 0.09\n",
      "\n",
      "==================== result of vector model [ '4월_10일_투표' ] ====================\n",
      "Precision: 0.04, Recall: 0.67\n",
      "F1 Score: 0.08\n",
      "\n",
      "Average Precision of boolean model: 0.84\n",
      "Average Recall of boolean model: 0.30\n",
      "Average F1 Score of boolean model: 0.41\n",
      "\n",
      "Average Precision of vector model: 0.42\n",
      "Average Recall of vector model: 0.72\n",
      "Average F1 Score of vector model: 0.41\n"
     ]
    }
   ],
   "source": [
    "_, _, results = evaluate_1(ground_truths, boolean_results, vector_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "76380249-e543-4ba6-af3f-180f3252a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRelevanceScorer:\n",
    "    def __init__(self, vector_results, recent=10, articles=30):\n",
    "        self.vector_results = vector_results\n",
    "        self.recent = recent\n",
    "        self.articles = articles\n",
    "        \n",
    "    def calculate_time_relevance(self, publication_date):\n",
    "        publication_datetime = datetime.strptime(publication_date, '%Y/%m/%d')\n",
    "        reference_datetime = datetime(2024, 4, 10)\n",
    "        \n",
    "        # Calculate days since publication\n",
    "        days_since_publication = (reference_datetime - publication_datetime).days\n",
    "        \n",
    "        # Return 1 if within recent days, otherwise 0\n",
    "        if days_since_publication <= int(self.recent):\n",
    "            return 1  \n",
    "        else:\n",
    "            return 0  \n",
    "\n",
    "    def time_relevance_scores(self, keys):\n",
    "        self.time_relevance_scores = {}\n",
    "\n",
    "        for key in keys:\n",
    "            if key in self.vector_results:\n",
    "                articles = self.vector_results[key]  # Get list of articles for the key\n",
    "                \n",
    "                key_time_relevance_scores = []  # Initialize relevance scores list\n",
    "\n",
    "                for article in articles:\n",
    "                    article_date = article['date']\n",
    "                    if article_date:  # Ensure date is not None\n",
    "                        time_bool = self.calculate_time_relevance(article_date)\n",
    "                        key_time_relevance_scores.append(time_bool)\n",
    "                \n",
    "                # Calculate average time relevance score\n",
    "                if key_time_relevance_scores:\n",
    "                    recent_article_ratio = sum(key_time_relevance_scores) / len(key_time_relevance_scores)\n",
    "                else:\n",
    "                    recent_article_ratio = 0\n",
    "                \n",
    "                self.time_relevance_scores[key] = recent_article_ratio\n",
    "\n",
    "        return self.time_relevance_scores\n",
    "    \n",
    "    def calculate_overall_average_score(self):        \n",
    "        all_scores = [score for score in self.time_relevance_scores.values()]\n",
    "        \n",
    "        if all_scores: \n",
    "            overall_average_score = sum(all_scores) / len(all_scores)\n",
    "        else:\n",
    "            overall_average_score = 0  \n",
    "\n",
    "        return overall_average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "941e90a5-8db1-49b2-bdeb-760303a7ea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'총선''s time relevance score : 0.1523\n",
      "'선거''s time relevance score : 0.2611\n",
      "'제22대_국회의원선거''s time relevance score : 0.2461\n",
      "'4월_10일_투표''s time relevance score : 0.5809\n",
      "\n",
      "overall average time relevance score: 0.3101\n"
     ]
    }
   ],
   "source": [
    "keys = ['총선', '선거', '제22대_국회의원선거', '4월_10일_투표']\n",
    "scorer = TimeRelevanceScorer(vector_results)\n",
    "average_time_relevance_scores = scorer.time_relevance_scores(keys)\n",
    "\n",
    "for key, average_score in average_time_relevance_scores.items():\n",
    "    print(f\"'{key}'\\'s time relevance score : {average_score:.4f}\")\n",
    "\n",
    "overall_average_score = scorer.calculate_overall_average_score()\n",
    "print(f\"\\noverall average time relevance score: {overall_average_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e2d6276-e22b-4808-9dfd-f4ef8e16d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Metric for '총선': 0.6801\n",
      "Combined Metric for '선거': 0.6276\n",
      "Combined Metric for '제22대_국회의원선거': 0.1685\n",
      "Combined Metric for '4월_10일_투표': 0.1268\n",
      "\n",
      "Overall average combined score: 0.40\n"
     ]
    }
   ],
   "source": [
    "def calculate_combined_metric(f1_score, time_relevance_score, alpha=0.7, beta=0.3):\n",
    "    combined_score = alpha * f1_score + beta * time_relevance_score\n",
    "    return combined_score\n",
    "\n",
    "f1 = [f[-1] for f in results[1]]\n",
    "f1_score = f1\n",
    "combined_results = []\n",
    "\n",
    "for i, key in enumerate(ground_truths.keys()):\n",
    "    combined_metric = calculate_combined_metric(f1[i], average_time_relevance_scores[key], alpha=0.9, beta=0.1)\n",
    "    print(f\"Combined Metric for '{key}': {combined_metric:.4f}\")\n",
    "    combined_results.append(combined_metric)\n",
    "\n",
    "average_combined = sum([result for result in combined_results]) / len(combined_results)\n",
    "print(f'\\nOverall average combined score: {average_combined:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
